{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "para = '''Thank you all so very much. Thank you to the Academy. Thank you to all of you in this room. I have to congratulate the other incredible nominees this year. The Revenant was the product of the tireless efforts of an unbelievable cast and crew. First off, to my brother in this endeavor, Mr. Tom Hardy. Tom, your talent on screen can only be surpassed by your friendship off screen … thank you for creating a transcendent cinematic experience. Thank you to everybody at Fox and New Regency … my entire team. I have to thank everyone from the very onset of my career … To my parents; none of this would be possible without you. And to my friends, I love you dearly; you know who you are.\n",
    "\n",
    "And lastly, I just want to say this: Making The Revenant was about man's relationship to the natural world. A world that we collectively felt in 2015 as the hottest year in recorded history. Our production needed to move to the southern tip of this planet just to be able to find snow. Climate change is real, it is happening right now. It is the most urgent threat facing our entire species, and we need to work collectively together and stop procrastinating. We need to support leaders around the world who do not speak for the big polluters, but who speak for all of humanity, for the indigenous people of the world, for the billions and billions of underprivileged people out there who would be most affected by this. For our children’s children, and for those people out there whose voices have been drowned out by the politics of greed. I thank you all for this amazing award tonight. Let us not take this planet for granted. I do not take tonight for granted. Thank you so very much.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Thank you all so very much. Thank you to the Academy. Thank you to all of you in this room. I have to congratulate the other incredible nominees this year. The Revenant was the product of the tireless efforts of an unbelievable cast and crew. First off, to my brother in this endeavor, Mr. Tom Hardy. Tom, your talent on screen can only be surpassed by your friendship off screen … thank you for creating a transcendent cinematic experience. Thank you to everybody at Fox and New Regency … my entire team. I have to thank everyone from the very onset of my career … To my parents; none of this would be possible without you. And to my friends, I love you dearly; you know who you are.\\n\\nAnd lastly, I just want to say this: Making The Revenant was about man's relationship to the natural world. A world that we collectively felt in 2015 as the hottest year in recorded history. Our production needed to move to the southern tip of this planet just to be able to find snow. Climate change is real, it is happening right now. It is the most urgent threat facing our entire species, and we need to work collectively together and stop procrastinating. We need to support leaders around the world who do not speak for the big polluters, but who speak for all of humanity, for the indigenous people of the world, for the billions and billions of underprivileged people out there who would be most affected by this. For our children’s children, and for those people out there whose voices have been drowned out by the politics of greed. I thank you all for this amazing award tonight. Let us not take this planet for granted. I do not take tonight for granted. Thank you so very much.\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "para"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization Of Paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sen = nltk.sent_tokenize(para)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Thank you to the Academy.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Sen[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = nltk.word_tokenize(para)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "346"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming and lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer,WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'para' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-b54a7aadb951>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpara\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'para' is not defined"
     ]
    }
   ],
   "source": [
    "para"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming pick each words and do stemming on each word.\n",
    "ps = PorterStemmer()\n",
    "\n",
    "Conv_to_sentence= nltk.sent_tokenize(para)\n",
    "\n",
    "for i in range(len(Conv_to_sentence)):\n",
    "    Conv_to_word = nltk.word_tokenize(Conv_to_sentence[i])\n",
    "    new_words = [ps.stem(words) for words in Conv_to_word]\n",
    "    Conv_to_sentence[i] = ' '.join(new_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatization pick each words and do lemmztization on each word. \n",
    "\n",
    "lem = WordNetLemmatizer()\n",
    "\n",
    "sent = nltk.sent_tokenize(para)\n",
    "for i in range(len(sent)):\n",
    "    word = nltk.word_tokenize(sent[i])\n",
    "    new_word = [lem.lemmatize(words) for words in word]\n",
    "    sent[i] = ' '.join(new_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## StopWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It helps us to remove stopwords from out sentences.\n",
    "nltk.download('stopwords') # IF any new stopword is added\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "senti = nltk.sent_tokenize(para)\n",
    "\n",
    "for i in range(len(senti)):\n",
    "    words = nltk.word_tokenize(senti[i])\n",
    "    New_Words = [x for x in words if x not in stopwords.words('english')]\n",
    "    senti[i] = ' '.join(New_Words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([i for i in stopwords.words('english')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = nltk.word_tokenize(para)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged = nltk.pos_tag(words) # It assign each word to part of speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it will convert in paragraph from tuple \n",
    "new = []\n",
    "for i in tagged:\n",
    "    new.append(i[0]+'_'+i[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag= ' '.join(new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BAG OF WORD Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Thank you all so very much. Thank you to the Academy. Thank you to all of you in this room. I have to congratulate the other incredible nominees this year. The Revenant was the product of the tireless efforts of an unbelievable cast and crew. First off, to my brother in this endeavor, Mr. Tom Hardy. Tom, your talent on screen can only be surpassed by your friendship off screen … thank you for creating a transcendent cinematic experience. Thank you to everybody at Fox and New Regency … my entire team. I have to thank everyone from the very onset of my career … To my parents; none of this would be possible without you. And to my friends, I love you dearly; you know who you are.\\n\\nAnd lastly, I just want to say this: Making The Revenant was about man's relationship to the natural world. A world that we collectively felt in 2015 as the hottest year in recorded history. Our production needed to move to the southern tip of this planet just to be able to find snow. Climate change is real, it is happening right now. It is the most urgent threat facing our entire species, and we need to work collectively together and stop procrastinating. We need to support leaders around the world who do not speak for the big polluters, but who speak for all of humanity, for the indigenous people of the world, for the billions and billions of underprivileged people out there who would be most affected by this. For our children’s children, and for those people out there whose voices have been drowned out by the politics of greed. I thank you all for this amazing award tonight. Let us not take this planet for granted. I do not take tonight for granted. Thank you so very much.\""
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "para"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import heapq\n",
    "import re\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocess the dataset\n",
    "dataset = nltk.sent_tokenize(para)\n",
    "\n",
    "for data in range(len(dataset)):\n",
    "    dataset[data] = dataset[data].lower()\n",
    "    dataset[data] = re.sub(r'\\W',' ',dataset[data])\n",
    "    dataset[data] = re.sub(r'\\s+',' ',dataset[data])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dictionary with count of each word in sentence , Also named as histogram\n",
    "word_count = {}\n",
    "for data in dataset:\n",
    "    for word in nltk.word_tokenize(data):\n",
    "        if word not in word_count.keys():\n",
    "            word_count[word] = 1\n",
    "        else:\n",
    "            word_count[word] +=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find out most used words in dictionary\n",
    "frequent_words = heapq.nlargest(100,word_count,word_count.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create BOW, where x matrix is BOW\n",
    "\n",
    "x = []\n",
    "for data in dataset:\n",
    "    vector = []\n",
    "    for word in frequent_words:\n",
    "        if word in nltk.word_tokenize(data):\n",
    "            vector.append(1)\n",
    "        else:\n",
    "            vector.append(0)\n",
    "    x.append(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.asarray(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All steps are same untill we get the ntarget number from whole corpus and then we create TF and IDF matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Thank you all so very much. Thank you to the Academy. Thank you to all of you in this room. I have to congratulate the other incredible nominees this year. The Revenant was the product of the tireless efforts of an unbelievable cast and crew. First off, to my brother in this endeavor, Mr. Tom Hardy. Tom, your talent on screen can only be surpassed by your friendship off screen … thank you for creating a transcendent cinematic experience. Thank you to everybody at Fox and New Regency … my entire team. I have to thank everyone from the very onset of my career … To my parents; none of this would be possible without you. And to my friends, I love you dearly; you know who you are.\\n\\nAnd lastly, I just want to say this: Making The Revenant was about man's relationship to the natural world. A world that we collectively felt in 2015 as the hottest year in recorded history. Our production needed to move to the southern tip of this planet just to be able to find snow. Climate change is real, it is happening right now. It is the most urgent threat facing our entire species, and we need to work collectively together and stop procrastinating. We need to support leaders around the world who do not speak for the big polluters, but who speak for all of humanity, for the indigenous people of the world, for the billions and billions of underprivileged people out there who would be most affected by this. For our children’s children, and for those people out there whose voices have been drowned out by the politics of greed. I thank you all for this amazing award tonight. Let us not take this planet for granted. I do not take tonight for granted. Thank you so very much.\""
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "para"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocess the dataset\n",
    "dataset = nltk.sent_tokenize(para)\n",
    "\n",
    "for data in range(len(dataset)):\n",
    "    dataset[data] = dataset[data].lower()\n",
    "    dataset[data] = re.sub(r'\\W',' ',dataset[data])\n",
    "    dataset[data] = re.sub(r'\\s+',' ',dataset[data])\n",
    "    \n",
    "# create dictionary with count of each word in sentence , Also named as histogram\n",
    "word_count = {}\n",
    "for data in dataset:\n",
    "    for word in nltk.word_tokenize(data):\n",
    "        if word not in word_count.keys():\n",
    "            word_count[word] = 1\n",
    "        else:\n",
    "            word_count[word] +=1\n",
    "\n",
    "# Find out most used words in dictionary\n",
    "frequent_words = heapq.nlargest(100,word_count,word_count.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IDF Matrix\n",
    "idf = {}\n",
    "for word in frequent_words:\n",
    "    No_of_document_contain_specific_word = 0\n",
    "    for data in dataset:\n",
    "        if word in nltk.word_tokenize(data):\n",
    "            No_of_document_contain_specific_word +=1\n",
    "    idf[word] = np.log(len(dataset)/No_of_document_contain_specific_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF Matrix\n",
    "\n",
    "tf= {}\n",
    "for word in frequent_words:\n",
    "    vector = []\n",
    "    for data in dataset:\n",
    "        frquency_of_word_in_document = 0\n",
    "        for w in nltk.word_tokenize(data):\n",
    "            if w == word:\n",
    "                frquency_of_word_in_document +=1\n",
    "                \n",
    "        tf_word =  frquency_of_word_in_document/len(nltk.word_tokenize(data))\n",
    "        vector.append(tf_word)\n",
    "    tf[word] = vector\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF Matrix , We multiply both matrix in this\n",
    "\n",
    "tf_idf = []\n",
    "for word in tf.keys():\n",
    "    tf_idf_for_single_word = []\n",
    "    for value in tf[word]:\n",
    "        score = value * idf[word]\n",
    "        tf_idf_for_single_word.append(score)\n",
    "    tf_idf.append(tf_idf_for_single_word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transpose and convert it to 2-d array\n",
    "\n",
    "X = np.asarray(tf_idf)\n",
    "\n",
    "X = np.transpose(X)\n",
    "\n",
    "X.shape #This is our TF-IDF matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-Gram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''The Delhi government on Friday expanded the list of Covid-19 hotspots in the city to bring three more localities'''\n",
    "z = '''a desperate effort to stop the spread of the disease that has infected more than 900 people.'''\n",
    "# The decision to include 3 more localities - Nabi Karim and Chandni Mahal in central Delhi and parts of Zakir Nagar in south - was driven by reports that 183 more people had tested positive.\n",
    "# Most of them, a whopping 154, had attended the Tablighi Jamaat congregation at a seven-storey building last month or were among the 2,000-plus evacuated from there. It had taken the intervention of National Security Adviser Ajit Doval to get them to vacate the building.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Delhi government on Friday expanded the list of Covid-19 hotspots in the city to bring three more localities'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Character N gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The list of Covid-19 hots in the Delhi government on Friday expanded the list on Friday expanded three \n"
     ]
    }
   ],
   "source": [
    "n= 3  # Vary this value of n to check the result. Higher N will give you better result\n",
    "ngram = {}\n",
    "\n",
    "#Create N-grams\n",
    "for i in range(len(text)-n):\n",
    "    gram = text[i:i+n] # text[0,3] means The ( First Trigram is The)\n",
    "    if gram not in ngram.keys():\n",
    "        ngram[gram] = []\n",
    "    ngram[gram].append(text[i+n]) # text[0+3] mean append next word after this Ngram, which is space.\n",
    "    \n",
    "\n",
    "\n",
    "#Testing our N Gram model #Build application\n",
    "import random\n",
    "currentGram = text[0:n]\n",
    "result = currentGram\n",
    "\n",
    "for i in range(100): # we build sentence with only 100 words\n",
    "    if currentGram not in ngram.keys():\n",
    "        break\n",
    "    possibilities = ngram[currentGram]\n",
    "    nextitem = possibilities[random.randrange(len(possibilities))]\n",
    "    result += nextitem\n",
    "    currentGram = result[len(result)-n:len(result)]\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word N-Gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Delhi government on Friday expanded the list of Covid-19 hotspots in the city to bring three more localities\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import nltk\n",
    "\n",
    "n= 3\n",
    "ngram = {}\n",
    "\n",
    "words = nltk.word_tokenize(text)\n",
    "for i in range(len(words)-n):\n",
    "    grams = ' '.join(words[i:i+n])\n",
    "    if grams not in ngram.keys():\n",
    "        ngram[grams] = []\n",
    "    ngram[grams].append(words[i+n])\n",
    "\n",
    "\n",
    "\n",
    "#Build application\n",
    "currentgram = ' '.join(words[0:n])\n",
    "result = currentgram\n",
    "\n",
    "for i in range(100):\n",
    "    if currentgram not in ngram.keys():\n",
    "        break\n",
    "    possibilities = ngram[currentgram]\n",
    "    nextitem = possibilities[random.randrange(len(possibilities))]\n",
    "    result += ' '+nextitem\n",
    "    rword = nltk.word_tokenize(result)\n",
    "    currentgram = ' '.join(rword[len(rword)-n:len(rword)])\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Semantic Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# Sample Data\n",
    "dataset = [\"The amount of population is increase day by day\",\"The Concert was just great\"\n",
    "          ,\"I love to see gomsay ramson cook\",\"Google is introducing a new technology\",\n",
    "          \"AI robots are example of great technology\",\"All of us singing in conert\",\n",
    "           \"We have lanuch campaign to stop polution and global warming\",\n",
    "          \"Sachin is very good crickets and he score much run\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The amount of population is increase day by day',\n",
       " 'The Concert was just great',\n",
       " 'I love to see gomsay ramson cook',\n",
       " 'Google is introducing a new technology',\n",
       " 'AI robots are example of great technology',\n",
       " 'All of us singing in conert',\n",
       " 'We have lanuch campaign to stop polution and global warming',\n",
       " 'Sachin is very good crickets and he score much run']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocess\n",
    "dataset = [i.lower() for i in dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create BOW model ( TFIDF)\n",
    "\n",
    "Vectorizer = TfidfVectorizer()\n",
    "X = Vectorizer.fit_transform(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 5)\t0.3202827550012799\n",
      "  (0, 11)\t0.6405655100025598\n",
      "  (0, 21)\t0.3202827550012799\n",
      "  (0, 23)\t0.2316261148762927\n",
      "  (0, 31)\t0.3202827550012799\n",
      "  (0, 29)\t0.2316261148762927\n",
      "  (0, 2)\t0.3202827550012799\n",
      "  (0, 41)\t0.2684219450882318\n"
     ]
    }
   ],
   "source": [
    "print(X[0]) # TFIDF value for 5th word in 0th Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TruncatedSVD(algorithm='randomized', n_components=4, n_iter=100,\n",
       "             random_state=None, tol=0.0)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert TFIDF matrix into 3 composed matrix\n",
    "\n",
    "lsa = TruncatedSVD(n_components=4,n_iter=100)\n",
    "lsa.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 48)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsa.components_.shape # 4 are no of concepts, 48 is no of diff words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.19868733, 0.10023544, 0.13200338, 0.06453788, 0.19868733,\n",
       "       0.13200338, 0.01608008, 0.16878611, 0.10023544, 0.0063397 ,\n",
       "       0.06092692, 0.26400676, 0.19868733, 0.01608008, 0.0063397 ,\n",
       "       0.06092692, 0.18215385, 0.30797142, 0.01608008, 0.06092692,\n",
       "       0.10023544, 0.13200338, 0.18215385, 0.27125808, 0.16878611,\n",
       "       0.01608008, 0.0063397 , 0.06092692, 0.18215385, 0.3116426 ,\n",
       "       0.01608008, 0.13200338, 0.0063397 , 0.19868733, 0.06092692,\n",
       "       0.06092692, 0.06092692, 0.0063397 , 0.10023544, 0.01608008,\n",
       "       0.31917464, 0.25208507, 0.01878952, 0.10023544, 0.06092692,\n",
       "       0.01608008, 0.16878611, 0.01608008])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsa.components_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Publish the most important word for each concept\n",
    "\n",
    "concept_words = {}\n",
    "terms = Vectorizer.get_feature_names() # GEt important words\n",
    "\n",
    "for i,comp in enumerate(lsa.components_):\n",
    "    componentTerm = zip(terms,comp)\n",
    "    SortedTerm = sorted(componentTerm,key= lambda x:x[1],reverse=True)\n",
    "    SortedTerm = SortedTerm[:10]\n",
    "    concept_words['Concept'+str(i)]= SortedTerm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in concept_words.keys():\n",
    "    sentence_Score = []\n",
    "    for sentence in dataset:\n",
    "        score = 0\n",
    "        for word in nltk.word_tokenize(sentence):\n",
    "            for word_With_score in concept_words[key]:\n",
    "                if word == word_With_score[0]: # it is a tuple\n",
    "                    score += word_With_score[1] #f word matches than append its score for that sentence\n",
    "        sentence_Score.append(score)\n",
    "    print(\"\\n\"+ key+ \":\")\n",
    "    for sentence_scores in sentence_Score:\n",
    "        print(sentence_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find Synonyms and antonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cool', 'aplomb', 'assuredness', 'cool', 'poise', 'sang-froid', 'cool', 'chill', 'cool_down', 'cool', 'chill', 'cool_down', 'cool', 'cool_off', 'cool_down', 'cool', 'cool', 'coolheaded', 'nerveless', 'cool', 'cool', 'cool', 'cool']\n",
      "['heat', 'heat', 'warm', 'warm', 'warm']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "synonyms = []\n",
    "antonyms = []\n",
    "\n",
    "\n",
    "\n",
    "for i in wordnet.synsets('cool'): # Change the word here.\n",
    "    for s in i.lemmas():\n",
    "        synonyms.append(s.name())\n",
    "        for a in s.antonyms():\n",
    "            antonyms.append(a.name())\n",
    "\n",
    "print(synonyms)\n",
    "print(antonyms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Negation Tracking in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am not_happy with our team performance\n"
     ]
    }
   ],
   "source": [
    "# tracking of nagative word in sentence like \"Not\"\n",
    "# Replace Not happy with not_happy.\n",
    "\n",
    "train = \"I am not happy with our team performance\"\n",
    "import nltk\n",
    "\n",
    "new_words = []\n",
    "temp_Word = \"\"\n",
    "\n",
    "for word in nltk.word_tokenize(train):\n",
    "    if word == 'not':\n",
    "        temp_Word =\"not_\"\n",
    "    elif temp_Word == \"not_\":\n",
    "        word = temp_Word+word\n",
    "        temp_Word = \"\"\n",
    "    if word != 'not':\n",
    "        new_words.append(word)\n",
    "    \n",
    "sentence  = ' '.join(new_words)\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am unhappy with our team performance\n"
     ]
    }
   ],
   "source": [
    "# Replace Not happy with antonyms of happy.\n",
    "train = \"I am not happy with our team performance\"\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "new_words = []\n",
    "temp_Word = \"\"\n",
    "\n",
    "for word in nltk.word_tokenize(train):\n",
    "    antonyms = []\n",
    "    if word == 'not':\n",
    "        temp_Word = 'not_'\n",
    "    elif temp_Word == 'not_':\n",
    "        for sys in wordnet.synsets(word):\n",
    "            for a in sys.lemmas():\n",
    "                for i in a.antonyms():\n",
    "                    antonyms.append(i.name())\n",
    "        if len(antonyms) >=1:\n",
    "            word = antonyms[0]\n",
    "        else:\n",
    "            word = temp_Word+word\n",
    "        temp_Word =\"\"\n",
    "    if word != 'not':\n",
    "        new_words.append(word)\n",
    "        \n",
    "sentence = ' '.join(new_words)\n",
    "print(sentence)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
